{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZBejXmsmWHp"
      },
      "source": [
        "# DiffFit-pytorch\n",
        "<a href=\"https://colab.research.google.com/github/mkshing/difffit-pytorch/blob/main/scripts/difffit_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This is an implementation of [DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2304.06648) by using dðŸ§¨ffusers. \n",
        "\n",
        "- My summary tweet: https://twitter.com/mk1stats/status/1647246562765012993\n",
        "- Code: https://github.com/mkshing/DiffFit-pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS5ixNcIgmOz"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtFrddhZfK0L"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!git clone https://mkshing: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@github.com/mkshing/difffit-pytorch.git\n",
        "!pip install -r difffit-pytorch/requirements.txt\n",
        "!pip install -q -U --pre triton\n",
        "!pip install -q ftfy bitsandbytes==0.35.0 gradio natsort xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8JJNJNRaG1kB"
      },
      "outputs": [],
      "source": [
        "# @markdown **(Optional) Login wandb**<br> If you don't use wandb for logging, make sure to remove `--report_to=\"wandb\"`\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIe4z_vfgsi2"
      },
      "source": [
        "## **Training DiffFit**\n",
        "\n",
        "In this example, use 5 dog images as usual by downloading from [here](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IHf8RTP_MMxg"
      },
      "outputs": [],
      "source": [
        "#@title **Dataset**\n",
        "import datetime\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def display_pic(folder):\n",
        "    fig = plt.figure(figsize=(30, 60))\n",
        "    files = sorted(glob.glob(folder+'/*.jpg'))\n",
        "    for i, file in enumerate(files):\n",
        "        img = Image.open(file)    \n",
        "        images = np.asarray(img)\n",
        "        ax = fig.add_subplot(10, 5, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(images)\n",
        "        ax.imshow(image_plt)\n",
        "        name = os.path.basename(file)\n",
        "        ax.set_xlabel(name, fontsize=30)  \n",
        "        fig.tight_layout()             \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# save_image = True #@param {type:\"boolean\"}\n",
        "mount_google_drive = True #@param {type:\"boolean\"}\n",
        "INSTANCE_DATA_DIR = \"/content/drive/MyDrive/AI/dreambooth-dog/data\" #@param {type: 'string'}\n",
        "CLASS_DATA_DIR = \"/content/drive/MyDrive/AI/dreambooth-dog/class-data\" #@param {type: 'string'}\n",
        "OUTPUT_DIR = \"/content/DiffFitOutput\" #@param {type: 'string'}\n",
        "\n",
        "if CLASS_DATA_DIR is None:\n",
        "  CLASS_DATA_DIR = OUTPUT_DIR + \"/class_data_dir\"\n",
        "\n",
        "force_remount = False\n",
        "if mount_google_drive:\n",
        "    from google.colab import drive # type: ignore\n",
        "    try:\n",
        "        drive_path = \"/content/drive\"\n",
        "        drive.mount(drive_path, force_remount=force_remount)\n",
        "        # output_path_gdrive = f\"/content/drive/MyDrive/{save_dir}\"\n",
        "        # save_dir = output_path_gdrive\n",
        "    except:\n",
        "        print(\"...error mounting drive or with drive path variables\")\n",
        "        print(\"...reverting to default path variables\")\n",
        "OUTPUT_DIR = os.path.abspath(OUTPUT_DIR)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"INSTANCE_DATA_DIR: {INSTANCE_DATA_DIR}\")\n",
        "print(f\"CLASS_DATA_DIR: {CLASS_DATA_DIR}\")\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S63rQW873HSa"
      },
      "outputs": [],
      "source": [
        "# @title **Parameters:**\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" # @param {type: \"string\"}\n",
        "# this is the number nitrosoke recommends \n",
        "NUM_CLASS_IMAGES = 200 #@param {type: \"integer\"}\n",
        "MAX_TRAIN_STEPS = 500 #@param {type: \"integer\"}\n",
        "CHECKPOINTING_STEPS = 100 #@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_a1wDgpQ1bZ"
      },
      "outputs": [],
      "source": [
        "from accelerate.utils import write_basic_config\n",
        "write_basic_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe-GgtnUVO_e"
      },
      "outputs": [],
      "source": [
        "# @title **Train:**\n",
        "! accelerate launch difffit-pytorch/train_difffit.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DATA_DIR \\\n",
        "  --class_data_dir=$CLASS_DATA_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --instance_prompt=\"photo of a sks dog\" \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=5e-4 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
        "  --checkpointing_steps=$CHECKPOINTING_STEPS \\\n",
        "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
        "  --use_8bit_adam \\\n",
        "  --seed=42 \\\n",
        "  --enable_xformers_memory_efficient_attention \\\n",
        "  --gradient_checkpointing \\\n",
        "  --add_vlb_loss \\\n",
        "  --vlb_lambda=0.001 \\\n",
        "  # --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  # --class_prompt=\"photo of a dog\" \\\n",
        "  # --train_text_encoder \\\n",
        "  # --report_to=\"wandb\" \\\n",
        "  # --bitfit \\\n",
        "  # --revision=\"fp16\" \\\n",
        "  # --mixed_precision=\"fp16\" \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4N3IVFyfMFO"
      },
      "source": [
        "### **Inference:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bMqSgMCqQvlC"
      },
      "outputs": [],
      "source": [
        "#@markdown **helper functions**\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "import requests\n",
        "import PIL\n",
        "import torch\n",
        "from torch import autocast\n",
        "import huggingface_hub\n",
        "from transformers import CLIPTextModel\n",
        "from diffusers import (\n",
        "    LMSDiscreteScheduler, \n",
        "    DDIMScheduler, \n",
        "    PNDMScheduler,\n",
        "    DPMSolverMultistepScheduler, \n",
        "    EulerDiscreteScheduler, \n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    StableDiffusionPipeline\n",
        ")\n",
        "from PIL import Image\n",
        "sys.path.append(\"/content/difffit-pytorch\")\n",
        "from difffit_pytorch.utils import load_unet_for_difffit, load_text_encoder_for_difffit, load_config_for_difffit\n",
        "\n",
        "\n",
        "SCHEDULER_MAPPING = {\n",
        "    \"ddim\": DDIMScheduler,\n",
        "    \"plms\": PNDMScheduler,\n",
        "    \"lms\": LMSDiscreteScheduler,\n",
        "    \"euler\": EulerDiscreteScheduler,\n",
        "    \"euler_ancestral\": EulerAncestralDiscreteScheduler,\n",
        "    \"dpm_solver++\": DPMSolverMultistepScheduler,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pfFRqt_SfN06"
      },
      "outputs": [],
      "source": [
        "# @markdown **Load model:**\n",
        "import sys\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "efficient_weights_ckpt = \"/content/DiffFitOutput/checkpoint-400\" #@param {type:\"string\"}\n",
        "scheduler_type = \"dpm_solver++\" #@param [\"ddim\", \"plms\", \"lms\", \"euler\", \"euler_ancestral\", \"dpm_solver++\"]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "training_args = load_config_for_difffit(efficient_weights_ckpt)\n",
        "unet = load_unet_for_difffit(MODEL_NAME, efficient_weights_ckpt=efficient_weights_ckpt, is_bitfit=training_args[\"bitfit\"] ,subfolder=\"unet\")\n",
        "text_encoder = load_text_encoder_for_difffit(MODEL_NAME, efficient_weights_ckpt=efficient_weights_ckpt, is_bitfit=training_args[\"bitfit\"] ,subfolder=\"text_encoder\")\n",
        "\n",
        "# load pipe\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    unet=unet,\n",
        "    text_encoder=text_encoder,\n",
        "    vae=AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\"),\n",
        "    requires_safety_checker=False,\n",
        "    safety_checker=None,\n",
        "    feature_extractor=None,\n",
        "    scheduler=SCHEDULER_MAPPING[scheduler_type].from_pretrained(MODEL_NAME, subfolder=\"scheduler\"),\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipe.to(device)\n",
        "print(\"loaded pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "# @markdown **Run!:**\n",
        "# @markdown <br> *It takes time at the 1st run because SVD is performed. \n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "prompt = \"A picture of a sks dog in a bucket\" #@param {type:\"string\"}\n",
        "num_images_per_prompt = 2 # @param {type: \"integer\"}\n",
        "guidance_scale = 7.5 # @param {type: \"number\"}\n",
        "num_inference_steps = 25 # @param {type: \"integer\"}\n",
        "height = 512 # @param {type: \"integer\"}\n",
        "width = 512 # @param {type: \"integer\"}\n",
        "seed = \"random_seed\" #@param {type:\"string\"}\n",
        "\n",
        "if seed == \"random_seed\":\n",
        "  random.seed()\n",
        "  seed = random.randint(0, 2**32)\n",
        "else:\n",
        "  seed = int(seed)\n",
        "g_cuda = torch.Generator(device='cuda').manual_seed(seed)\n",
        "print(f\"seed: {seed}\")\n",
        "\n",
        "prompts = prompt.split(\"::\")\n",
        "all_images = []\n",
        "for prompt in tqdm(prompts):\n",
        "    with torch.autocast(device), torch.inference_mode():\n",
        "        images = pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            generator=g_cuda\n",
        "        ).images\n",
        "    all_images.extend(images)\n",
        "grid_image = image_grid(all_images, len(prompts), num_images_per_prompt)\n",
        "grid_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkJ4Gmhjjn5b"
      },
      "source": [
        "### **(Optional) Upload HuggingFace Hub**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qYY90LQqPfys"
      },
      "outputs": [],
      "source": [
        "# @markdown login huggingface hub\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zYK3sawnhgnF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "\n",
        "hub_model_id = \"\" #@param {type: \"string\"}\n",
        "hub_token = \"\" #@param {type: \"string\"}\n",
        "folder_path = \"/content/DiffFitOutput\" #@param {type: \"string\"}\n",
        "\n",
        "if hub_token == \"\":\n",
        "  hub_token = None\n",
        "repo_id = create_repo(repo_id=hub_model_id, exist_ok=True, token=hub_token).repo_id\n",
        "\n",
        "base_model = MODEL_NAME\n",
        "instance_prompt = \"photo of a sks dog\"  #@param {type: \"string\"}\n",
        "# @markdown paste your `instace_prompt` here.\n",
        "\n",
        "yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {base_model}\n",
        "instance_prompt: {instance_prompt}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- difffit\n",
        "inference: true\n",
        "---\n",
        "\"\"\"\n",
        "model_card = f\"\"\"\n",
        "# DiffFit - {repo_id}\n",
        "These are DiffFit weights for {base_model}. The weights were trained on {instance_prompt}.\"\"\"\n",
        "with open(os.path.join(folder_path, \"README.md\"), \"w\") as f:\n",
        "    f.write(yaml + model_card)\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=folder_path,\n",
        "    commit_message=\"first commit\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AkJ4Gmhjjn5b"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
